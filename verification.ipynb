{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import tensorboardX\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # scipy throws future warnings on fft (known bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2spectrogram(path, segment_len=3, window='hamming', Tw=25, Ts=10, \n",
    "                    pre_emphasis=0.97, alpha=0.99, return_onesided=False):\n",
    "    # read .wav file\n",
    "    rate, samples = wavfile.read(path)\n",
    "    \n",
    "    ## parameters\n",
    "    # frame duration (samples)\n",
    "    Nw = int(rate * Tw * 1e-3)\n",
    "    Ns = int(rate * (Tw - Ts) * 1e-3)\n",
    "    # overlapped duration (samples)\n",
    "    # 2 ** to the next pow of 2 of (Nw - 1)\n",
    "    nfft = 2 ** (Nw - 1).bit_length()\n",
    "\n",
    "    # preemphasis filter\n",
    "    samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
    "\n",
    "    # removes DC component of the signal and add a small dither\n",
    "    samples = signal.lfilter([1, -1], [1, -alpha], samples)\n",
    "    dither = np.random.uniform(-1, 1, samples.shape)\n",
    "    spow = np.std(samples)\n",
    "    samples = samples + 1e-6 * spow * dither\n",
    "\n",
    "    # segment selection\n",
    "    upper_bound = len(samples) - segment_len * rate\n",
    "    start = np.random.randint(0, upper_bound)\n",
    "    end = start + segment_len * rate\n",
    "    samples = samples[start:end]\n",
    "\n",
    "    # spectogram\n",
    "    _, _, spec = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
    "                                    mode='magnitude', return_onesided=return_onesided)\n",
    "\n",
    "    # just multiplying it by 1600 makes spectrograms in the paper and here \"the same\"\n",
    "    spec *= rate / 10\n",
    "    \n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentificationDatasetTrain(Dataset):\n",
    "    \n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        \n",
    "        iden_split_path = os.path.join(path, 'iden_split.txt')\n",
    "        split = pd.read_table(iden_split_path, sep=' ', header=None, names=['phase', 'path'])\n",
    "        split['label'] = split['path'].apply(lambda x: int(x.split('/')[0].replace('id1', '')) - 1)\n",
    "        \n",
    "        # make train/test id split (in paths class id numbering starts with 1)\n",
    "        fullid_arr = np.arange(1251) # 1--1251\n",
    "        testid_arr = np.arange(269, 309) # 270--309\n",
    "        trainid_arr = np.setdiff1d(fullid_arr, testid_arr) # 1--1251 \\ 270--309\n",
    "        # subsetting ids for training\n",
    "        mask = split['label'].isin(trainid_arr)\n",
    "        self.dataset = split['path'][mask].reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # path\n",
    "        track_path = self.dataset[idx]\n",
    "        audio_path = os.path.join(self.path, 'audio', track_path)\n",
    "        \n",
    "        # extract label from path like id10003/L9_sh8msGV59/00001.txt\n",
    "        # subtracting 1 because PyTorch assumes that C_i in [0, 1251-1]\n",
    "        label = int(track_path.split('/')[0].replace('id1', '')) - 1\n",
    "        # PyTorch complains if label > num_classes. For ex, num_classes=1211\n",
    "        # label is 1250. train labels \\in [0, ..., 268, 309, ..., 1250]. (269 + 942 = 1211)\n",
    "        # therefore, we subtract 40 (# of test classes) from a label => label \\in [0, 1211]\n",
    "        if label >= 309:\n",
    "            label -= 40\n",
    "        \n",
    "        # make a spectrogram from a .wavfile\n",
    "        spec = wav2spectrogram(audio_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            spec = self.transform(spec)\n",
    "\n",
    "        return label, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"Normalizes voice spectrogram (mean-varience)\"\"\"\n",
    "    \n",
    "    def __call__(self, spec):\n",
    "        \n",
    "        # (Freq, Time)\n",
    "        # mean-variance normalization for every spectrogram (not batch-wise)\n",
    "        mu = spec.mean(axis=1).reshape(512, 1)\n",
    "        sigma = spec.std(axis=1).reshape(512, 1)\n",
    "        spec = (spec - mu) / sigma\n",
    "\n",
    "        return spec\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert spectogram to Tensor.\"\"\"\n",
    "    \n",
    "    def __call__(self, spec):\n",
    "        F, T = spec.shape\n",
    "        \n",
    "        # now specs are of size (Freq, Time) and 2D but has to be 3D (channel dim)\n",
    "        spec = spec.reshape(1, F, T)\n",
    "        \n",
    "        # make the ndarray to be of a proper type (was float64)\n",
    "        spec = spec.astype(np.float32)\n",
    "        \n",
    "        return torch.from_numpy(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(VoiceNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=7, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(num_features=96)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn6 = nn.BatchNorm2d(num_features=4096)\n",
    "        self.bn7 = nn.BatchNorm1d(num_features=1024)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.mpool5 = nn.MaxPool2d(kernel_size=(5, 3), stride=(3, 2))\n",
    "        \n",
    "        # Conv2d with weights of size (H, 1) is identical to FC with H weights\n",
    "        self.fc6 = nn.Conv2d(in_channels=256, out_channels=4096, kernel_size=(9, 1))\n",
    "        self.fc7 = nn.Linear(in_features=4096, out_features=1024)\n",
    "        self.fc8 = nn.Linear(in_features=1024, out_features=num_classes)\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.mpool1(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.mpool2(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.mpool5(x)\n",
    "        x = self.relu(self.bn6(self.fc6(x)))\n",
    "        \n",
    "        _, _, _, W = x.size()\n",
    "        self.apool6 = nn.AvgPool2d(kernel_size=(1, W))\n",
    "        x = self.apool6(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.relu(self.bn7(self.fc7(x)))\n",
    "            x = self.fc8(x)\n",
    "        \n",
    "        # we use the fc7 output for Hard Negative Mining (inference)\n",
    "        else:\n",
    "            x = self.fc7(x)\n",
    "            x = F.normalize(x)\n",
    "        \n",
    "        # during training, there's no need for SoftMax because CELoss calculates it\n",
    "        return x\n",
    "    \n",
    "    # phase: [training_iden, inference_negative_mining, training_siamese, verif_test]\n",
    "    def forward(self, voice1, voice2=None, phase='train_iden'):\n",
    "        if phase in ['train_iden', 'eval_mining']:\n",
    "            return self.forward_once(voice1)\n",
    "        \n",
    "        elif phase in ['train_veri', 'eval_veri']:\n",
    "            voice1 = self.forward_once(voice1)\n",
    "            voice2 = self.forward_once(voice2)\n",
    "            return voice1, voice2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/home/nvme/data/vc1/'\n",
    "LOG_PATH = '/home/vladimir/nvme/logs/VoxCeleb/verif_class'\n",
    "EPOCH_NUM = 30\n",
    "\n",
    "# in shared code B = 100 but PyTorch throws CUDA out of memory at B = 97 \n",
    "# though B=96 takes only 90.6% of the GPU Mem (bug?):\n",
    "# https://discuss.pytorch.org/t/lesser-memory-consumption-with-a-larger-batch-in-multi-gpu-setup/29087\n",
    "# B = 96\n",
    "# but when \n",
    "torch.backends.cudnn.deterministic = True\n",
    "# I can set B = 100\n",
    "B = 100\n",
    "\n",
    "WEIGHT_DECAY = 5e-4\n",
    "LR_INIT = 1e-2\n",
    "LR_LAST = 1e-4\n",
    "# lr scheduler parameter\n",
    "gamma = 10 ** (np.log10(LR_LAST / LR_INIT) / (EPOCH_NUM - 1))\n",
    "MOMENTUM = 0.9\n",
    "DEVICE = 'cuda:0'\n",
    "NUM_WORKERS = 4\n",
    "TBoard = tensorboardX.SummaryWriter(log_dir=LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VoiceNet(num_classes=1211)\n",
    "net.to(DEVICE)\n",
    "\n",
    "transforms = Compose([\n",
    "    Normalize(),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "trainset = IdentificationDatasetTrain(DATASET_PATH, transform=transforms)\n",
    "trainsetloader = torch.utils.data.DataLoader(trainset, batch_size=B, \n",
    "                                             num_workers=NUM_WORKERS, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), LR_INIT, MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1487it [07:20,  4.08it/s]\n",
      "1487it [07:25,  4.10it/s]\n",
      "1487it [07:25,  4.09it/s]\n",
      "1487it [07:25,  4.09it/s]\n",
      "1487it [07:25,  4.09it/s]\n",
      "1487it [07:25,  4.08it/s]\n",
      "1487it [07:25,  4.08it/s]\n",
      "1487it [07:25,  4.02it/s]\n",
      "1487it [07:25,  4.10it/s]\n",
      "1487it [07:25,  4.09it/s]\n",
      "1487it [07:25,  4.03it/s]\n",
      "1487it [07:25,  4.08it/s]\n",
      "1487it [07:24,  4.09it/s]\n",
      "1487it [07:25,  4.10it/s]\n",
      "18it [00:06,  3.33it/s]Process Process-59:\n",
      "Process Process-57:\n",
      "Process Process-60:\n",
      "Process Process-58:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-5-47f8540fcf47>\", line 52, in __getitem__\n",
      "    samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"<ipython-input-5-47f8540fcf47>\", line 57, in __getitem__\n",
      "    spow = np.std(samples)\n",
      "  File \"<ipython-input-5-47f8540fcf47>\", line 69, in __getitem__\n",
      "    mode='magnitude', return_onesided=False)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/scipy/signal/spectral.py\", line 707, in spectrogram\n",
      "    mode='stft')\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/numpy/core/fromnumeric.py\", line 3038, in std\n",
      "    **kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/scipy/signal/spectral.py\", line 1641, in _spectral_helper\n",
      "    result = _fft_helper(x, win, detrend_func, nperseg, noverlap, nfft, sides)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/numpy/core/_methods.py\", line 140, in _std\n",
      "    keepdims=keepdims)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/scipy/signal/spectral.py\", line 1726, in _fft_helper\n",
      "    result = func(result, n=nfft)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/numpy/core/_methods.py\", line 117, in _var\n",
      "    x = asanyarray(arr - arrmean)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/scipy/fftpack/basic.py\", line 283, in fft\n",
      "    return work_function(tmp,n,1,0,overwrite_x)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b64ee9128846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# TBoard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mstep_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_num\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainsetloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0miter_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mTBoard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Metrics/train_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mTBoard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Metrics/lr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_num in range(EPOCH_NUM):\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # train\n",
    "    net.train()\n",
    "    \n",
    "    for iter_num, (labels, specs) in tqdm(enumerate(trainsetloader)):\n",
    "        optimizer.zero_grad()\n",
    "        labels, specs = labels.to(DEVICE), specs.to(DEVICE)\n",
    "        scores = net(specs)\n",
    "        loss = criterion(scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # TBoard\n",
    "        step_num = epoch_num * len(trainsetloader) + iter_num\n",
    "        TBoard.add_scalar('Metrics/train_loss', loss.item(), step_num)\n",
    "        TBoard.add_scalar('Metrics/lr', lr_scheduler.get_lr()[0], step_num)\n",
    "        \n",
    "# when the training is finished save the model\n",
    "torch.save(net.state_dict(), os.path.join(LOG_PATH, 'model_snapshot_{}.txt'.format(time.time())))\n",
    "TBoard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), os.path.join(LOG_PATH, 'model_snapshot_{}.txt'.format(time.time())))\n",
    "TBoard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/vladimir/nvme/logs/VoxCeleb/verif_class/model_snapshot_1542979501.519298.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-33c230e54901>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpretrained_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOG_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_snapshot_1542979501.519298.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVoiceNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1211\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/vladimir/nvme/logs/VoxCeleb/verif_class/model_snapshot_1542979501.519298.txt'"
     ]
    }
   ],
   "source": [
    "pretrained_dict = torch.load(os.path.join(LOG_PATH, 'model_snapshot_1542979501.519298.txt'))\n",
    "\n",
    "net = VoiceNet(num_classes=1211)\n",
    "net.to(DEVICE)\n",
    "\n",
    "model_dict = net.state_dict()\n",
    "\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict) \n",
    "# 3. load the new state dict\n",
    "net.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1211])\n",
      "torch.Size([10, 1024])\n",
      "torch.Size([10, 1024])\n",
      "torch.Size([10, 1024])\n",
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "B = 10\n",
    "\n",
    "net.train()\n",
    "v1 = torch.rand((B, 1, 512, 298)).to(DEVICE)\n",
    "print(net(v1, phase='train_iden').shape) # B, 1211\n",
    "\n",
    "net.eval()\n",
    "v1 = torch.rand((B, 1, 512, 298)).to(DEVICE)\n",
    "v2 = torch.rand((B, 1, 512, 298)).to(DEVICE)\n",
    "print(net(v1, phase='eval_mining').shape) # B, 1024\n",
    "print(net(v2, phase='eval_mining').shape) # B, 1024\n",
    "\n",
    "net.train()\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "net.fc8 = nn.Linear(net.fc8.in_features, 1024).to(DEVICE)\n",
    "v1 = torch.rand((B, 1, 512, 298)).to(DEVICE)\n",
    "v2 = torch.rand((B, 1, 512, 298)).to(DEVICE)\n",
    "print(net(v1, v2, phase='train_veri')[1].shape) # B, 1024\n",
    "\n",
    "net.eval()\n",
    "v1 = torch.rand((1, 1, 512, 2324)).to(DEVICE)\n",
    "v2 = torch.rand((1, 1, 512, 3245)).to(DEVICE)\n",
    "print(net(v1, v2, phase='eval_veri')[1].shape) # 1, 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1211\n",
      "1240\n"
     ]
    }
   ],
   "source": [
    "fullid_arr = np.arange(1251) # 1--1251\n",
    "testid_arr = np.arange(269, 309) # 270--309\n",
    "trainid_arr = np.setdiff1d(fullid_arr, testid_arr)\n",
    "print(len(trainid_arr))\n",
    "print(trainid_arr[1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardNegativeMining(Dataset):\n",
    "    \n",
    "    def __init__(self, path, model, transform=None):\n",
    "        self.path = path\n",
    "        self.model = model\n",
    "        self.transform = transform\n",
    "        \n",
    "        fullid_arr = np.arange(1251) # 1--1251\n",
    "        testid_arr = np.arange(269, 309) # 270--309\n",
    "        trainid_arr = np.setdiff1d(fullid_arr, testid_arr) # 1--1251 \\ 270--309\n",
    "        self.dataset = trainid_arr\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.dataset\n",
    "#         return (<B positives + B/2 random negatives + B/2 10% hardest> x 1 x 512 x 298)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullid_arr = np.arange(1, 1252) # 1--1251\n",
    "testid_arr = np.arange(270, 310) # 270--309\n",
    "trainid_arr = np.setdiff1d(fullid_arr, testid_arr) # 1--1251 \\ 270--309\n",
    "# a = np.random.choice(trainid_arr, 1211, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "         27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "         40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "         53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "         66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "         79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101]),\n",
       " array([102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n",
       "        115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127,\n",
       "        128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
       "        141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166,\n",
       "        167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179,\n",
       "        180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192,\n",
       "        193, 194, 195, 196, 197, 198, 199, 200, 201, 202]),\n",
       " array([203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215,\n",
       "        216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228,\n",
       "        229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,\n",
       "        242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254,\n",
       "        255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267,\n",
       "        268, 269, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320,\n",
       "        321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333,\n",
       "        334, 335, 336, 337, 338, 339, 340, 341, 342, 343]),\n",
       " array([344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356,\n",
       "        357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369,\n",
       "        370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382,\n",
       "        383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395,\n",
       "        396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408,\n",
       "        409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421,\n",
       "        422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
       "        435, 436, 437, 438, 439, 440, 441, 442, 443, 444]),\n",
       " array([445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457,\n",
       "        458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470,\n",
       "        471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483,\n",
       "        484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496,\n",
       "        497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
       "        510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522,\n",
       "        523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535,\n",
       "        536, 537, 538, 539, 540, 541, 542, 543, 544, 545]),\n",
       " array([546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
       "        559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
       "        572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
       "        585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
       "        598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
       "        611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
       "        624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n",
       "        637, 638, 639, 640, 641, 642, 643, 644, 645, 646]),\n",
       " array([647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659,\n",
       "        660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n",
       "        673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
       "        686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698,\n",
       "        699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711,\n",
       "        712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724,\n",
       "        725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737,\n",
       "        738, 739, 740, 741, 742, 743, 744, 745, 746, 747]),\n",
       " array([748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760,\n",
       "        761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773,\n",
       "        774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786,\n",
       "        787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799,\n",
       "        800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812,\n",
       "        813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825,\n",
       "        826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838,\n",
       "        839, 840, 841, 842, 843, 844, 845, 846, 847, 848]),\n",
       " array([849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861,\n",
       "        862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874,\n",
       "        875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887,\n",
       "        888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900,\n",
       "        901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913,\n",
       "        914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926,\n",
       "        927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939,\n",
       "        940, 941, 942, 943, 944, 945, 946, 947, 948, 949]),\n",
       " array([ 950,  951,  952,  953,  954,  955,  956,  957,  958,  959,  960,\n",
       "         961,  962,  963,  964,  965,  966,  967,  968,  969,  970,  971,\n",
       "         972,  973,  974,  975,  976,  977,  978,  979,  980,  981,  982,\n",
       "         983,  984,  985,  986,  987,  988,  989,  990,  991,  992,  993,\n",
       "         994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003, 1004,\n",
       "        1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015,\n",
       "        1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026,\n",
       "        1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037,\n",
       "        1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048,\n",
       "        1049, 1050]),\n",
       " array([1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061,\n",
       "        1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072,\n",
       "        1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083,\n",
       "        1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094,\n",
       "        1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105,\n",
       "        1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116,\n",
       "        1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127,\n",
       "        1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138,\n",
       "        1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149,\n",
       "        1150, 1151]),\n",
       " array([1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162,\n",
       "        1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173,\n",
       "        1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184,\n",
       "        1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195,\n",
       "        1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206,\n",
       "        1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217,\n",
       "        1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228,\n",
       "        1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239,\n",
       "        1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250,\n",
       "        1251])]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_split(trainid_arr, 1211 // 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationDatasetTrain(Dataset):\n",
    "    \n",
    "    def __init__(self, path, model, batch_size, device, transform=None):\n",
    "        self.path = path\n",
    "        self.model = model\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        \n",
    "        fullid_arr = np.arange(1, 1252) # 1--1251\n",
    "        testid_arr = np.arange(270, 310) # 270--309\n",
    "        trainid_arr = np.setdiff1d(fullid_arr, testid_arr) # 1--1251 \\ 270--309\n",
    "\n",
    "        # split the set of ids into `len(trainid_arr) // batch_size` subsets\n",
    "        self.splits = np.array_split(trainid_arr, len(trainid_arr) // batch_size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.splits)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        ## POSITIVE PART\n",
    "        ids = self.splits[idx]\n",
    "        anchors = [0] * len(ids)\n",
    "        positives = [0] * len(ids)\n",
    "        \n",
    "        for i, id in enumerate(ids):\n",
    "            # 265 -> id10265\n",
    "            full_id = 'id1{:04d}'.format(id)\n",
    "            # list all tracks for that id\n",
    "            track_list = os.listdir(os.path.join(self.path, 'audio', full_id))\n",
    "            # randomly select two tracks without replacement\n",
    "            track1, track2 = np.random.choice(track_list, 2, replace=False)\n",
    "            # select two voice segments\n",
    "            voice1_path = os.path.join(self.path, 'audio', full_id, track1)\n",
    "            voice2_path = os.path.join(self.path, 'audio', full_id, track2)\n",
    "            # create spectrograms for selected .wav files\n",
    "            spec1 = wav2spectrogram(voice1_path) \n",
    "            spec2 = wav2spectrogram(voice2_path)\n",
    "            # add to the list\n",
    "            anchors[i] = spec1\n",
    "            positives[i] = spec2\n",
    "            \n",
    "        anchors = torch.cat(anchors).to(self.device)\n",
    "        positives = torch.cat(positives).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        anchors = self.model(anchors, 'eval_mining') # B, 1024\n",
    "        positives = self.model(positives, 'eval_mining') # --//---\n",
    "        \n",
    "        dists = (anchors ** 2).sum(dim=1).view(-1, 1) + \\\n",
    "                (positives ** 2).sum(dim=1) - \\\n",
    "                2 * anchors.matmul(positives.t())\n",
    "        dists = torch.sqrt(dists)\n",
    "        \n",
    "        # divide the current set of ids into two parts\n",
    "        negatives_hnm, negatives_rand = np.array_split(ids, 2)\n",
    "        \n",
    "        ## HARD NEGATIVE MINING PART\n",
    "        \n",
    "        \n",
    "        ##  RANDOM PART\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(self_splits[0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 79  80  85  93  12  23  39  37  52  58  82  27  17  42  56  44   3  19\n",
      "  70  45  28  36   1  66  49  54  31  71  61  22  90  10  95  46  84  91\n",
      "  75  53   9   5   6  87  88  78  92  77  62  73  99  41  55  51  30  26\n",
      "  63  59  47   2  68  43  35  96  33  48  25  65  89  11  57  50  38  32\n",
      "  24 101  13  64  83  15  14  76  29  74   4  40  81  86 100  20   8  98\n",
      "  21  72  34  67  18  97  69  94  16   7  60]\n",
      "[ 51  30  26  63  59  47   2  68  43  35  96  33  48  25  65  89  11  57\n",
      "  50  38  32  24 101  13  64  83  15  14  76  29  74   4  40  81  86 100\n",
      "  20   8  98  21  72  34  67  18  97  69  94  16   7  60]\n"
     ]
    }
   ],
   "source": [
    "fullid_arr = np.arange(1, 1252) # 1--1251\n",
    "testid_arr = np.arange(270, 310) # 270--309\n",
    "trainid_arr = np.setdiff1d(fullid_arr, testid_arr) # 1--1251 \\ 270--309\n",
    "\n",
    "# split the set of ids into `len(trainid_arr) // batch_size` subsets\n",
    "self_splits = np.array_split(trainid_arr, 1211 // 100)\n",
    "ids = np.random.permutation(self_splits[0])\n",
    "print(ids)\n",
    "negatives_rand, negatives_hnm = np.array_split(ids, 2)\n",
    "print(negatives_hnm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 3\n",
    "anchors = torch.randint(high=10, size=(B, 10))\n",
    "positives = torch.randint(high=10, size=(B, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 7., 5., 1., 0., 1., 6., 7., 7., 5.],\n",
      "        [6., 9., 5., 7., 0., 0., 6., 2., 3., 7.],\n",
      "        [8., 2., 2., 6., 0., 0., 5., 8., 7., 0.]])\n",
      "\n",
      "tensor([[9., 1., 0., 5., 7., 7., 5., 5., 7., 7.],\n",
      "        [7., 3., 6., 7., 8., 5., 6., 0., 5., 0.],\n",
      "        [0., 2., 4., 3., 6., 0., 9., 1., 5., 2.]])\n",
      "tensor([[14.0000, 14.8324, 11.8743],\n",
      "        [15.0333, 13.5647, 13.3041],\n",
      "        [12.7671, 13.3041, 13.6382]])\n",
      "tensor([[11.8743, 14.0000, 14.8324],\n",
      "        [13.3041, 13.5647, 15.0333],\n",
      "        [12.7671, 13.3041, 13.6382]]) tensor([[2, 0, 1],\n",
      "        [2, 1, 0],\n",
      "        [0, 1, 2]])\n",
      "tensor([[2, 1],\n",
      "        [2, 0],\n",
      "        [0, 1]])\n",
      "tensor([2, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "print(anchors)\n",
    "print()\n",
    "print(positives)\n",
    "dists = (anchors**2).sum(dim=1).view(-1, 1) + (positives**2).sum(dim=1) - 2*anchors.matmul(positives.t())\n",
    "dists = np.sqrt(dists)\n",
    "print(dists)\n",
    "dists_sorted, dists_sorted_idx = dists.sort(dim=1)\n",
    "print(dists_sorted, dists_sorted_idx)\n",
    "idx_threshold = round(tau * (B-1))\n",
    "# Given a distance matrix Dij, if i=j a value corresponds to a distance between \n",
    "# positive pairs -> we need to prevent them from getting to the negative samples\n",
    "# First, we need to remove i=j elements.\n",
    "mask = (dists_sorted_idx != torch.arange(B).repeat(1, B).view(B, B).t())\n",
    "dists_sorted_idx_rm = dists_sorted_idx[mask].view(B, B-1)\n",
    "print(dists_sorted_idx_rm)\n",
    "selected_idx = dists_sorted_idx_rm[:, idx_threshold]\n",
    "print(selected_idx)\n",
    "# tau = 0.1\n",
    "# position = round(tau * (B-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
