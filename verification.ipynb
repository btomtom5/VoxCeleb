{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import tensorboardX\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationDatasetTrain(Dataset):\n",
    "    \n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        \n",
    "        iden_split_path = os.path.join(path, 'iden_split.txt')\n",
    "        split = pd.read_table(iden_split_path, sep=' ', header=None, names=['phase', 'path'])\n",
    "        split['label'] = split['path'].apply(lambda x: int(x.split('/')[0].replace('id1', '')) - 1)\n",
    "        \n",
    "        # make train/test id split (in paths class id numbering starts with 1)\n",
    "        fullid_arr = np.arange(1251) # 1--1251\n",
    "        testid_arr = np.arange(269, 309) # 270--309\n",
    "        trainid_arr = np.setdiff1d(fullid_arr, testid_arr) # 1--1251 \\ 270--309\n",
    "        # subsetting ids for training\n",
    "        mask = split['label'].isin(trainid_arr)\n",
    "        self.dataset = split['path'][mask].reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # path\n",
    "        track_path = self.dataset[idx]\n",
    "        audio_path = os.path.join(self.path, 'audio', track_path)\n",
    "\n",
    "        # read .wav\n",
    "        rate, samples = wavfile.read(audio_path)\n",
    "        # extract label from path like id10003/L9_sh8msGV59/00001.txt\n",
    "        # subtracting 1 because PyTorch assumes that C_i in [0, 1251-1]\n",
    "        label = int(track_path.split('/')[0].replace('id1', '')) - 1\n",
    "\n",
    "        ## parameters\n",
    "        window = 'hamming'\n",
    "        # window width and step size\n",
    "        Tw = 25 # ms\n",
    "        Ts = 10 # ms\n",
    "        # frame duration (samples)\n",
    "        Nw = int(rate * Tw * 1e-3)\n",
    "        Ns = int(rate * (Tw - Ts) * 1e-3)\n",
    "        # overlapped duration (samples)\n",
    "        # 2 ** to the next pow of 2 of (Nw - 1)\n",
    "        nfft = 2 ** (Nw - 1).bit_length()\n",
    "        pre_emphasis = 0.97\n",
    "        \n",
    "        # preemphasis filter\n",
    "        samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
    "        \n",
    "        # removes DC component of the signal and add a small dither\n",
    "        samples = signal.lfilter([1, -1], [1, -0.99], samples)\n",
    "        dither = np.random.uniform(-1, 1, samples.shape)\n",
    "        spow = np.std(samples)\n",
    "        samples = samples + 1e-6 * spow * dither\n",
    "        \n",
    "        # segment selection\n",
    "        segment_len = 3 # sec\n",
    "        upper_bound = len(samples) - segment_len * rate\n",
    "        start = np.random.randint(0, upper_bound)\n",
    "        end = start + segment_len * rate\n",
    "        samples = samples[start:end]\n",
    "        \n",
    "        # spectogram\n",
    "        _, _, spec = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
    "                                        mode='magnitude', return_onesided=False)\n",
    "        \n",
    "        # just multiplying it by 1600 makes spectrograms in the paper and here \"the same\"\n",
    "        spec *= rate / 10\n",
    "        \n",
    "        if self.transform:\n",
    "            spec = self.transform(spec)\n",
    "\n",
    "        return label, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationDatasetTest(Dataset):\n",
    "    \n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        veri_split_path = os.path.join(path, 'veri_test.txt')\n",
    "        self.dataset = pd.read_table(veri_split_path, sep=' ', header=None, \n",
    "                                     names=['label', 'voice1', 'voice2'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # path\n",
    "        label, voice1_path, voice2_path = self.dataset.iloc[idx]\n",
    "        voice1_path = os.path.join(self.path, 'audio', voice1_path)\n",
    "        voice2_path = os.path.join(self.path, 'audio', voice2_path)\n",
    "\n",
    "        # read .wav\n",
    "        rate, voice1 = wavfile.read(voice1_path)\n",
    "        rate, voice2 = wavfile.read(voice2_path)\n",
    "\n",
    "        ## parameters\n",
    "        window = 'hamming'\n",
    "        # window width and step size\n",
    "        Tw = 25 # ms\n",
    "        Ts = 10 # ms\n",
    "        # frame duration (samples)\n",
    "        Nw = int(rate * Tw * 1e-3)\n",
    "        Ns = int(rate * (Tw - Ts) * 1e-3)\n",
    "        # overlapped duration (samples)\n",
    "        # 2 ** to the next pow of 2 of (Nw - 1)\n",
    "        nfft = 2 ** (Nw - 1).bit_length()\n",
    "        pre_emphasis = 0.97\n",
    "        \n",
    "        # preemphasis filter\n",
    "        voice1 = np.append(voice1[0], voice1[1:] - pre_emphasis * voice1[:-1])\n",
    "        voice2 = np.append(voice2[0], voice2[1:] - pre_emphasis * voice2[:-1])\n",
    "        \n",
    "        \n",
    "        # removes DC component of the signal and add a small dither\n",
    "        voice1 = signal.lfilter([1, -1], [1, -0.99], voice1)\n",
    "        voice2 = signal.lfilter([1, -1], [1, -0.99], voice2)\n",
    "        dither1 = np.random.uniform(-1, 1, voice1.shape)\n",
    "        dither2 = np.random.uniform(-1, 1, voice2.shape)\n",
    "        spow1 = np.std(voice1)\n",
    "        spow2 = np.std(voice2)\n",
    "        voice1 = voice1 + 1e-6 * spow1 * dither1\n",
    "        voice2 = voice2 + 1e-6 * spow2 * dither2\n",
    "        \n",
    "        # spectogram\n",
    "        _, _, spec1 = signal.spectrogram(voice1, rate, window, Nw, Ns, nfft, \n",
    "                                         mode='magnitude', return_onesided=False)\n",
    "        _, _, spec2 = signal.spectrogram(voice2, rate, window, Nw, Ns, nfft, \n",
    "                                         mode='magnitude', return_onesided=False)\n",
    "        \n",
    "        # just multiplying it by 1600 makes spectrograms in the paper and here \"the same\"\n",
    "        spec1 *= rate / 10\n",
    "        spec2 *= rate / 10\n",
    "        \n",
    "        if self.transform:\n",
    "            spec1 = self.transform(spec1)\n",
    "            spec2 = self.transform(spec2)\n",
    "\n",
    "        return label, spec1, spec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"Normalizes voice spectrogram (mean-varience)\"\"\"\n",
    "    \n",
    "    def __call__(self, spec):\n",
    "        \n",
    "        # (Freq, Time)\n",
    "        # mean-variance normalization for every spectrogram (not batch-wise)\n",
    "        mu = spec.mean(axis=1).reshape(512, 1)\n",
    "        sigma = spec.std(axis=1).reshape(512, 1)\n",
    "        spec = (spec - mu) / sigma\n",
    "\n",
    "        return spec\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert spectogram to Tensor.\"\"\"\n",
    "    \n",
    "    def __call__(self, spec):\n",
    "        F, T = spec.shape\n",
    "        \n",
    "        # now specs are of size (Freq, Time) and 2D but has to be 3D (channel dim)\n",
    "        spec = spec.reshape(1, F, T)\n",
    "        \n",
    "        # make the ndarray to be of a proper type (was float64)\n",
    "        spec = spec.astype(np.float32)\n",
    "        \n",
    "        return torch.from_numpy(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(VoiceNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=7, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(num_features=96)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn6 = nn.BatchNorm2d(num_features=4096)\n",
    "        self.bn7 = nn.BatchNorm1d(num_features=1024)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.mpool5 = nn.MaxPool2d(kernel_size=(5, 3), stride=(3, 2))\n",
    "        \n",
    "        # Conv2d with weights of size (H, 1) is identical to FC with H weights\n",
    "        self.fc6 = nn.Conv2d(in_channels=256, out_channels=4096, kernel_size=(9, 1))\n",
    "        self.fc7 = nn.Linear(in_features=4096, out_features=1024)\n",
    "        self.fc8 = nn.Linear(in_features=1024, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.mpool1(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.mpool2(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.mpool5(x)\n",
    "        x = self.relu(self.bn6(self.fc6(x)))\n",
    "        \n",
    "        _, _, _, W = x.size()\n",
    "        self.apool6 = nn.AvgPool2d(kernel_size=(1, W))\n",
    "        x = self.apool6(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.bn7(self.fc7(x)))\n",
    "        x = self.fc8(x)\n",
    "        \n",
    "        # during training, there's no need for SoftMax because CELoss calculates it\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/home/nvme/data/vc1/'\n",
    "LOG_PATH = '/home/nvme/logs/VoxCeleb/verif1'\n",
    "EPOCH_NUM = 30\n",
    "\n",
    "# in shared code B = 100 but PyTorch throws CUDA out of memory at B = 97 \n",
    "# though B=96 takes only 90.6% of the GPU Mem (bug?):\n",
    "# https://discuss.pytorch.org/t/lesser-memory-consumption-with-a-larger-batch-in-multi-gpu-setup/29087\n",
    "# B = 96\n",
    "# but when \n",
    "torch.backends.cudnn.deterministic = True\n",
    "# I can set B = 100\n",
    "B = 100\n",
    "\n",
    "WEIGHT_DECAY = 5e-4\n",
    "LR_INIT = 1e-2\n",
    "LR_LAST = 1e-4\n",
    "# lr scheduler parameter\n",
    "gamma = 10 ** (np.log10(LR_LAST / LR_INIT) / (EPOCH_NUM - 1))\n",
    "MOMENTUM = 0.9\n",
    "DEVICE = 'cuda:0'\n",
    "NUM_WORKERS = 4\n",
    "TBoard = tensorboardX.SummaryWriter(log_dir=LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VoiceNet(num_classes=1211)\n",
    "net.to(DEVICE)\n",
    "\n",
    "transforms = Compose([\n",
    "    Normalize(),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "trainset = VerificationDatasetTrain(DATASET_PATH, transform=transforms)\n",
    "trainsetloader = torch.utils.data.DataLoader(trainset, batch_size=B, \n",
    "                                             num_workers=NUM_WORKERS, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
