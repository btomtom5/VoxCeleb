{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import tensorboardX\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # scipy throws future warnings on fft (known bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2spectrogram(path, segment_len=3, window='hamming', Tw=25, Ts=10, \n",
    "                    pre_emphasis=0.97, alpha=0.99, return_onesided=False):\n",
    "    # read .wav file\n",
    "    rate, samples = wavfile.read(path)\n",
    "    \n",
    "    ## parameters\n",
    "    # frame duration (samples)\n",
    "    Nw = int(rate * Tw * 1e-3)\n",
    "    Ns = int(rate * (Tw - Ts) * 1e-3)\n",
    "    # overlapped duration (samples)\n",
    "    # 2 ** to the next pow of 2 of (Nw - 1)\n",
    "    nfft = 2 ** (Nw - 1).bit_length()\n",
    "\n",
    "    # preemphasis filter\n",
    "    samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
    "\n",
    "    # removes DC component of the signal and add a small dither\n",
    "    samples = signal.lfilter([1, -1], [1, -alpha], samples)\n",
    "    dither = np.random.uniform(-1, 1, samples.shape)\n",
    "    spow = np.std(samples)\n",
    "    samples = samples + 1e-6 * spow * dither\n",
    "\n",
    "    # segment selection\n",
    "    upper_bound = len(samples) - segment_len * rate\n",
    "    start = np.random.randint(0, upper_bound)\n",
    "    end = start + segment_len * rate\n",
    "    samples = samples[start:end]\n",
    "\n",
    "    # spectogram\n",
    "    _, _, spec = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
    "                                    mode='magnitude', return_onesided=return_onesided)\n",
    "\n",
    "    # just multiplying it by 1600 makes spectrograms in the paper and here \"the same\"\n",
    "    spec *= rate / 10\n",
    "    \n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentificationDatasetTrain(Dataset):\n",
    "    \n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        \n",
    "        iden_split_path = os.path.join(path, 'iden_split.txt')\n",
    "        split = pd.read_table(iden_split_path, sep=' ', header=None, names=['phase', 'path'])\n",
    "        split['label'] = split['path'].apply(lambda x: int(x.split('/')[0].replace('id1', '')) - 1)\n",
    "        \n",
    "        # make train/test id split (in paths class id numbering starts with 1)\n",
    "        fullid_arr = np.arange(1251) # 1--1251\n",
    "        testid_arr = np.arange(269, 309) # 270--309\n",
    "        trainid_arr = np.setdiff1d(fullid_arr, testid_arr) # 1--1251 \\ 270--309\n",
    "        # subsetting ids for training\n",
    "        mask = split['label'].isin(trainid_arr)\n",
    "        self.dataset = split['path'][mask].reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # path\n",
    "        track_path = self.dataset[idx]\n",
    "        audio_path = os.path.join(self.path, 'audio', track_path)\n",
    "        \n",
    "        # extract label from path like id10003/L9_sh8msGV59/00001.txt\n",
    "        # subtracting 1 because PyTorch assumes that C_i in [0, 1251-1]\n",
    "        label = int(track_path.split('/')[0].replace('id1', '')) - 1\n",
    "        # PyTorch complains if label > num_classes. For ex, num_classes=1211\n",
    "        # label is 1250. train labels \\in [0, ..., 268, 309, ..., 1250]. (269 + 942 = 1211)\n",
    "        # therefore, we subtract 40 (# of test classes) from a label => label \\in [0, 1211]\n",
    "        if label >= 309:\n",
    "            label -= 40\n",
    "        \n",
    "        # make a spectrogram from a .wavfile\n",
    "        spec = wav2spectrogram(audio_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            spec = self.transform(spec)\n",
    "\n",
    "        return label, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"Normalizes voice spectrogram (mean-varience)\"\"\"\n",
    "    \n",
    "    def __call__(self, spec):\n",
    "        \n",
    "        # (Freq, Time)\n",
    "        # mean-variance normalization for every spectrogram (not batch-wise)\n",
    "        mu = spec.mean(axis=1).reshape(512, 1)\n",
    "        sigma = spec.std(axis=1).reshape(512, 1)\n",
    "        spec = (spec - mu) / sigma\n",
    "\n",
    "        return spec\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert spectogram to Tensor.\"\"\"\n",
    "    \n",
    "    def __call__(self, spec):\n",
    "        F, T = spec.shape\n",
    "        \n",
    "        # now specs are of size (Freq, Time) and 2D but has to be 3D (channel dim)\n",
    "        spec = spec.reshape(1, F, T)\n",
    "        \n",
    "        # make the ndarray to be of a proper type (was float64)\n",
    "        spec = spec.astype(np.float32)\n",
    "        \n",
    "        return torch.from_numpy(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(VoiceNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=7, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(num_features=96)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn6 = nn.BatchNorm2d(num_features=4096)\n",
    "        self.bn7 = nn.BatchNorm1d(num_features=1024)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.mpool5 = nn.MaxPool2d(kernel_size=(5, 3), stride=(3, 2))\n",
    "        \n",
    "        # Conv2d with weights of size (H, 1) is identical to FC with H weights\n",
    "        self.fc6 = nn.Conv2d(in_channels=256, out_channels=4096, kernel_size=(9, 1))\n",
    "        self.fc7 = nn.Linear(in_features=4096, out_features=1024)\n",
    "        self.fc8 = nn.Linear(in_features=1024, out_features=num_classes)\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.mpool1(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.mpool2(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.mpool5(x)\n",
    "        x = self.relu(self.bn6(self.fc6(x)))\n",
    "        \n",
    "        _, _, _, W = x.size()\n",
    "        self.apool6 = nn.AvgPool2d(kernel_size=(1, W))\n",
    "        x = self.apool6(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.relu(self.bn7(self.fc7(x)))\n",
    "            x = self.fc8(x)\n",
    "        \n",
    "        # we use the fc7 output for Hard Negative Mining (inference)\n",
    "        else:\n",
    "            x = self.fc7(x)\n",
    "            x = F.normalize(x)\n",
    "        \n",
    "        # during training, there's no need for SoftMax because CELoss calculates it\n",
    "        return x\n",
    "    \n",
    "    # phase: [training_iden, inference_negative_mining, training_siamese, verif_test]\n",
    "    def forward(self, voice1, voice2=None, phase='train_iden'):\n",
    "        if phase in ['train_iden', 'eval_mining']:\n",
    "            return self.forward_once(voice1)\n",
    "        \n",
    "        elif phase in ['train_veri', 'eval_veri']:\n",
    "            voice1 = self.forward_once(voice1)\n",
    "            voice2 = self.forward_once(voice2)\n",
    "            return voice1, voice2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/home/nvme/data/vc1/'\n",
    "LOG_PATH = '/home/vladimir/nvme/logs/VoxCeleb/verif_class'\n",
    "EPOCH_NUM = 30\n",
    "\n",
    "# in shared code B = 100 but PyTorch throws CUDA out of memory at B = 97 \n",
    "# though B=96 takes only 90.6% of the GPU Mem (bug?):\n",
    "# https://discuss.pytorch.org/t/lesser-memory-consumption-with-a-larger-batch-in-multi-gpu-setup/29087\n",
    "# B = 96\n",
    "# but when \n",
    "torch.backends.cudnn.deterministic = True\n",
    "# I can set B = 100\n",
    "B = 100\n",
    "\n",
    "WEIGHT_DECAY = 5e-4\n",
    "LR_INIT = 1e-2\n",
    "LR_LAST = 1e-4\n",
    "# lr scheduler parameter\n",
    "gamma = 10 ** (np.log10(LR_LAST / LR_INIT) / (EPOCH_NUM - 1))\n",
    "MOMENTUM = 0.9\n",
    "DEVICE = 'cuda:0'\n",
    "NUM_WORKERS = 4\n",
    "TBoard = tensorboardX.SummaryWriter(log_dir=LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:74",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5096d523b3e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVoiceNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1211\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m transforms = Compose([\n\u001b[1;32m      5\u001b[0m     \u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:74"
     ]
    }
   ],
   "source": [
    "net = VoiceNet(num_classes=1211)\n",
    "net.to(DEVICE)\n",
    "\n",
    "transforms = Compose([\n",
    "    Normalize(),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "trainset = IdentificationDatasetTrain(DATASET_PATH, transform=transforms)\n",
    "trainsetloader = torch.utils.data.DataLoader(trainset, batch_size=B, \n",
    "                                             num_workers=NUM_WORKERS, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), LR_INIT, MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1487it [07:20,  4.08it/s]\n",
      "1487it [07:25,  4.10it/s]\n",
      "1487it [07:25,  4.09it/s]\n",
      "1487it [07:25,  4.09it/s]\n",
      "1487it [07:25,  4.09it/s]\n",
      "1487it [07:25,  4.08it/s]\n",
      "1487it [07:25,  4.08it/s]\n",
      "1487it [07:25,  4.02it/s]\n",
      "1487it [07:25,  4.10it/s]\n",
      "1487it [07:25,  4.09it/s]\n",
      "1487it [07:25,  4.03it/s]\n",
      "1487it [07:25,  4.08it/s]\n",
      "1487it [07:24,  4.09it/s]\n",
      "1487it [07:25,  4.10it/s]\n",
      "18it [00:06,  3.33it/s]Process Process-59:\n",
      "Process Process-57:\n",
      "Process Process-60:\n",
      "Process Process-58:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-5-47f8540fcf47>\", line 52, in __getitem__\n",
      "    samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"<ipython-input-5-47f8540fcf47>\", line 57, in __getitem__\n",
      "    spow = np.std(samples)\n",
      "  File \"<ipython-input-5-47f8540fcf47>\", line 69, in __getitem__\n",
      "    mode='magnitude', return_onesided=False)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/scipy/signal/spectral.py\", line 707, in spectrogram\n",
      "    mode='stft')\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/numpy/core/fromnumeric.py\", line 3038, in std\n",
      "    **kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/scipy/signal/spectral.py\", line 1641, in _spectral_helper\n",
      "    result = _fft_helper(x, win, detrend_func, nperseg, noverlap, nfft, sides)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/numpy/core/_methods.py\", line 140, in _std\n",
      "    keepdims=keepdims)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/scipy/signal/spectral.py\", line 1726, in _fft_helper\n",
      "    result = func(result, n=nfft)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/numpy/core/_methods.py\", line 117, in _var\n",
      "    x = asanyarray(arr - arrmean)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/scipy/fftpack/basic.py\", line 283, in fft\n",
      "    return work_function(tmp,n,1,0,overwrite_x)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b64ee9128846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# TBoard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mstep_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_num\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainsetloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0miter_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mTBoard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Metrics/train_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mTBoard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Metrics/lr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_num in range(EPOCH_NUM):\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # train\n",
    "    net.train()\n",
    "    \n",
    "    for iter_num, (labels, specs) in tqdm(enumerate(trainsetloader)):\n",
    "        optimizer.zero_grad()\n",
    "        labels, specs = labels.to(DEVICE), specs.to(DEVICE)\n",
    "        scores = net(specs)\n",
    "        loss = criterion(scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # TBoard\n",
    "        step_num = epoch_num * len(trainsetloader) + iter_num\n",
    "        TBoard.add_scalar('Metrics/train_loss', loss.item(), step_num)\n",
    "        TBoard.add_scalar('Metrics/lr', lr_scheduler.get_lr()[0], step_num)\n",
    "        \n",
    "# when the training is finished save the model\n",
    "torch.save(net.state_dict(), os.path.join(LOG_PATH, 'model_snapshot_{}.txt'.format(time.time())))\n",
    "TBoard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), os.path.join(LOG_PATH, 'model_snapshot_{}.txt'.format(time.time())))\n",
    "TBoard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/vladimir/nvme/logs/VoxCeleb/verif_class/model_snapshot_1542979501.519298.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-33c230e54901>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpretrained_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOG_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_snapshot_1542979501.519298.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVoiceNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1211\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/vladimir/nvme/logs/VoxCeleb/verif_class/model_snapshot_1542979501.519298.txt'"
     ]
    }
   ],
   "source": [
    "pretrained_dict = torch.load(os.path.join(LOG_PATH, 'model_snapshot_1542979501.519298.txt'))\n",
    "\n",
    "net = VoiceNet(num_classes=1211)\n",
    "net.to(DEVICE)\n",
    "\n",
    "model_dict = net.state_dict()\n",
    "\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict) \n",
    "# 3. load the new state dict\n",
    "net.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1211])\n",
      "torch.Size([10, 1024])\n",
      "torch.Size([10, 1024])\n",
      "torch.Size([10, 1024])\n",
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "B = 10\n",
    "\n",
    "net.train()\n",
    "v1 = torch.rand((B, 1, 512, 298)).to(DEVICE)\n",
    "print(net(v1, phase='train_iden').shape) # B, 1211\n",
    "\n",
    "net.eval()\n",
    "v1 = torch.rand((B, 1, 512, 298)).to(DEVICE)\n",
    "v2 = torch.rand((B, 1, 512, 298)).to(DEVICE)\n",
    "print(net(v1, phase='eval_mining').shape) # B, 1024\n",
    "print(net(v2, phase='eval_mining').shape) # B, 1024\n",
    "\n",
    "net.train()\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "net.fc8 = nn.Linear(net.fc8.in_features, 1024).to(DEVICE)\n",
    "v1 = torch.rand((B, 1, 512, 298)).to(DEVICE)\n",
    "v2 = torch.rand((B, 1, 512, 298)).to(DEVICE)\n",
    "print(net(v1, v2, phase='train_veri')[1].shape) # B, 1024\n",
    "\n",
    "net.eval()\n",
    "v1 = torch.rand((1, 1, 512, 2324)).to(DEVICE)\n",
    "v2 = torch.rand((1, 1, 512, 3245)).to(DEVICE)\n",
    "print(net(v1, v2, phase='eval_veri')[1].shape) # 1, 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationDatasetTrain(Dataset):\n",
    "    \n",
    "    def __init__(self, path, model, batch_size, device, transform=None):\n",
    "        self.path = path\n",
    "        self.model = model\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        \n",
    "        fullid_arr = np.arange(1, 1252) # 1--1251\n",
    "        testid_arr = np.arange(270, 310) # 270--309\n",
    "        trainid_arr = np.setdiff1d(fullid_arr, testid_arr) # 1--1251 \\ 270--309\n",
    "\n",
    "        # split the set of ids into `len(trainid_arr) // batch_size` subsets\n",
    "        self.splits = np.array_split(trainid_arr, len(trainid_arr) // batch_size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.splits)\n",
    "    \n",
    "    def cosine_sim_matrix(tensor1, tensor2):\n",
    "        B, D = tensor1.size()\n",
    "        dot = tensor2 @ tensor1.t()\n",
    "        norm1 = tensor1.norm(dim=1)\n",
    "        norm2 = tensor2.norm(dim=1).view(1, B).t()\n",
    "        dot /= norm1 * norm2\n",
    "        return dot.t()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        ## POSITIVE PART\n",
    "        ids = self.splits[idx]\n",
    "        # shuffle ids to make sure that every negative pair will consist of voices of \n",
    "        # different identities at each iteration.\n",
    "        ids = np.random.permutation(ids)\n",
    "        anchors = [0] * len(ids)\n",
    "        positives = [0] * len(ids)\n",
    "        \n",
    "        for i, id in enumerate(ids):\n",
    "            # 265 -> id10265\n",
    "            full_id = 'id1{:04d}'.format(id)\n",
    "            # list all tracks for that id\n",
    "            track_list = os.listdir(os.path.join(self.path, 'audio', full_id))\n",
    "            # randomly select two tracks without replacement\n",
    "            track1, track2 = np.random.choice(track_list, 2, replace=False)\n",
    "            # select two voice segments\n",
    "            voice1_path = os.path.join(self.path, 'audio', full_id, track1)\n",
    "            voice2_path = os.path.join(self.path, 'audio', full_id, track2)\n",
    "            # create spectrograms for selected .wav files\n",
    "            spec1 = wav2spectrogram(voice1_path) \n",
    "            spec2 = wav2spectrogram(voice2_path)\n",
    "            # add to the list\n",
    "            anchors[i] = spec1\n",
    "            positives[i] = spec2\n",
    "            \n",
    "        anchors = torch.cat(anchors).to(self.device)\n",
    "        positives = torch.cat(positives).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        anchors = self.model(anchors, 'eval_mining') # B, 1024\n",
    "        positives = self.model(positives, 'eval_mining') # --//---\n",
    "        \n",
    "        # calculate a cosine similarity matrix\n",
    "        sim_mat = cosine_sim_matrix(anchors, positives)\n",
    "        \n",
    "        \n",
    "        # divide the current set of ids into two parts\n",
    "        negatives_hnm, negatives_rand = np.array_split(ids, 2)\n",
    "        \n",
    "        ## HARD NEGATIVE MINING PART\n",
    "        \n",
    "        \n",
    "        ## RANDOM PART\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 80  13  66  37   7  76  10  62  21  90  47  61  72  45  48  81  94  49\n",
      "  67  71  44  33  26  55  42   2 101  38  60   5  93  27  36  20  59  24\n",
      "  85  88  70  79  74  19   1  58  57  63  34  28  83  78  87  50   8  97\n",
      "  51  17  52   4  56  99  18  84  89  54  75   6  14  43  68  77  31  82\n",
      "  40  98  29  35  53  12  11  46  86  64  41  65  22  96  25  69   3  95\n",
      "  73  91  39  15   9  16 100  30  23  92  32]\n",
      "[ 50   8  97  51  17  52   4  56  99  18  84  89  54  75   6  14  43  68\n",
      "  77  31  82  40  98  29  35  53  12  11  46  86  64  41  65  22  96  25\n",
      "  69   3  95  73  91  39  15   9  16 100  30  23  92  32]\n"
     ]
    }
   ],
   "source": [
    "fullid_arr = np.arange(1, 1252) # 1--1251\n",
    "testid_arr = np.arange(270, 310) # 270--309\n",
    "trainid_arr = np.setdiff1d(fullid_arr, testid_arr) # 1--1251 \\ 270--309\n",
    "\n",
    "# split the set of ids into `len(trainid_arr) // batch_size` subsets\n",
    "self_splits = np.array_split(trainid_arr, 1211 // 100)\n",
    "ids = np.random.permutation(self_splits[0])\n",
    "print(ids)\n",
    "negatives_rand, negatives_hnm = np.array_split(ids, 2)\n",
    "print(negatives_hnm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2210, -1.5022,  1.0385, -0.0150],\n",
      "        [ 1.9777, -0.3181,  0.0651, -1.0455],\n",
      "        [ 1.2548, -0.4052, -0.2234,  0.0043],\n",
      "        [ 1.6252,  0.2383,  0.6327,  0.9166],\n",
      "        [-0.3791, -0.5218, -0.2294, -1.8437]])\n",
      "\n",
      "tensor([[-1.2607,  3.0024,  1.4890, -0.3889],\n",
      "        [ 0.5755, -3.1204,  0.0870, -0.7363],\n",
      "        [-0.6371,  0.5573, -1.1816, -1.9365],\n",
      "        [-0.2033, -1.8213, -0.6009, -1.3229],\n",
      "        [ 0.3952, -1.6698,  0.2164,  0.5675]])\n",
      "torch.Size([5, 4]) torch.Size([5, 4])\n",
      "tensor([-0.4044,  0.3945, -0.2377, -0.5079, -0.1046])\n",
      "tensor([[-0.4044,  0.7777, -0.4253,  0.5060,  0.7876],\n",
      "        [-0.3617,  0.3945,  0.0933,  0.2877,  0.1782],\n",
      "        [-0.6504,  0.4507, -0.2377,  0.1955,  0.4628],\n",
      "        [-0.1046, -0.0662, -0.7128, -0.5079,  0.2495],\n",
      "        [-0.1007,  0.4288,  0.7963,  0.7836, -0.1046]])\n",
      "tensor([[-0.4044,  0.7777, -0.4253,  0.5060,  0.7876],\n",
      "        [-0.3617,  0.3945,  0.0933,  0.2877,  0.1782],\n",
      "        [-0.6504,  0.4507, -0.2377,  0.1955,  0.4628],\n",
      "        [-0.1046, -0.0662, -0.7128, -0.5079,  0.2495],\n",
      "        [-0.1007,  0.4288,  0.7963,  0.7836, -0.1046]])\n",
      "tensor([[-0.4044,  0.7777, -0.4253,  0.5060,  0.7876],\n",
      "        [-0.3617,  0.3945,  0.0933,  0.2877,  0.1782],\n",
      "        [-0.6504,  0.4507, -0.2377,  0.1955,  0.4628],\n",
      "        [-0.1046, -0.0662, -0.7128, -0.5079,  0.2495],\n",
      "        [-0.1007,  0.4288,  0.7963,  0.7836, -0.1046]])\n",
      "tensor(-0.0000)\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity_matrix_loop(tensor1, tensor2):\n",
    "    sim = [[F.cosine_similarity(a, b, dim=0) for a in tensor2] for b in tensor1]\n",
    "    return torch.Tensor(sim)\n",
    "\n",
    "def cosine_similarity_matrix_vectorized(tensor1, tensor2):\n",
    "    B, D = tensor1.size()\n",
    "    dot = tensor2 @ tensor1.t()\n",
    "    norm1 = tensor1.norm(dim=1)\n",
    "    norm2 = tensor2.norm(dim=1).view(1, B).t()\n",
    "    dot /= norm1 * norm2\n",
    "    return dot.t()\n",
    "\n",
    "B = 5\n",
    "\n",
    "anchors = torch.randn(B, 4)\n",
    "positives = torch.randn(B, 4)\n",
    "\n",
    "print(anchors)\n",
    "print()\n",
    "print(positives)\n",
    "print(anchors.size(), positives.size())\n",
    "\n",
    "print(F.cosine_similarity(anchors, positives))\n",
    "\n",
    "sim = [[F.cosine_similarity(a, b, dim=0) for a in positives] for b in anchors]\n",
    "sim = torch.Tensor(sim)\n",
    "print(sim)\n",
    "print(cosine_similarity_matrix_loop(anchors, positives))\n",
    "print(cosine_similarity_matrix_vectorized(anchors, positives))\n",
    "print((cosine_similarity_matrix_loop(anchors, positives) - cosine_similarity_matrix_vectorized(anchors, positives)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4044,  0.7777, -0.4253,  0.5060,  0.7876],\n",
      "        [-0.3617,  0.3945,  0.0933,  0.2877,  0.1782],\n",
      "        [-0.6504,  0.4507, -0.2377,  0.1955,  0.4628],\n",
      "        [-0.1046, -0.0662, -0.7128, -0.5079,  0.2495],\n",
      "        [-0.1007,  0.4288,  0.7963,  0.7836, -0.1046]])\n",
      "tensor([[-0.4253, -0.4044,  0.5060,  0.7777,  0.7876],\n",
      "        [-0.3617,  0.0933,  0.1782,  0.2877,  0.3945],\n",
      "        [-0.6504, -0.2377,  0.1955,  0.4507,  0.4628],\n",
      "        [-0.7128, -0.5079, -0.1046, -0.0662,  0.2495],\n",
      "        [-0.1046, -0.1007,  0.4288,  0.7836,  0.7963]])\n",
      "tensor([[2, 0, 3, 1, 4],\n",
      "        [0, 2, 4, 3, 1],\n",
      "        [0, 2, 3, 1, 4],\n",
      "        [2, 3, 0, 1, 4],\n",
      "        [4, 0, 1, 3, 2]])\n",
      "tensor([[2, 3, 1, 4],\n",
      "        [0, 2, 4, 3],\n",
      "        [0, 3, 1, 4],\n",
      "        [2, 0, 1, 4],\n",
      "        [0, 1, 3, 2]])\n",
      "tensor([0, 2, 0])\n",
      "tensor([[2],\n",
      "        [2]])\n",
      "torch.Size([3]) torch.Size([2, 1])\n",
      "tensor([2, 2, 0, 2, 0])\n",
      "tensor([[-0.6371,  0.5573, -1.1816, -1.9365],\n",
      "        [-0.6371,  0.5573, -1.1816, -1.9365],\n",
      "        [-1.2607,  3.0024,  1.4890, -0.3889],\n",
      "        [-0.6371,  0.5573, -1.1816, -1.9365],\n",
      "        [-1.2607,  3.0024,  1.4890, -0.3889]])\n"
     ]
    }
   ],
   "source": [
    "print(sim)\n",
    "sim_sorted, sim_sorted_idx = sim.sort(dim=1)\n",
    "print(sim_sorted)\n",
    "print(sim_sorted_idx)\n",
    "# Given a sim matrix Dij, if i=j a value corresponds to a similarity between \n",
    "# positive pairs -> we need to prevent them from getting to the negative samples\n",
    "# First, we need to remove i=j elements.\n",
    "mask = (sim_sorted_idx != torch.arange(B).repeat(1, B).view(B, B).t())\n",
    "sim_sorted_idx_rm = sim_sorted_idx[mask].view(B, B-1)\n",
    "print(sim_sorted_idx_rm)\n",
    "# select the indices for appropriately hard samples\n",
    "tau = 0.1\n",
    "idx_threshold = round(tau * (B-2))\n",
    "# only half of the batch size -> B // 2\n",
    "hnm_idxs = sim_sorted_idx_rm[B // 2:, idx_threshold]\n",
    "print(hnm_idxs)\n",
    "idx_threshold_rand = torch.from_numpy(np.random.uniform(size=(B, 1)) * (B-1)).long()\n",
    "# print(idx_threshold_rand)\n",
    "# rand_idxs = sim_sorted_idx_rm[:B // 2, idx_threshold_rand]\n",
    "rand_idxs = torch.gather(sim_sorted_idx_rm, dim=1, index=idx_threshold_rand)[:B // 2]\n",
    "print(rand_idxs)\n",
    "print(hnm_idxs.shape, rand_idxs.shape)\n",
    "print(torch.cat([rand_idxs.view(-1), hnm_idxs.view(-1)]))\n",
    "negatives = positives[torch.cat([rand_idxs.view(-1), hnm_idxs.view(-1)]), :]\n",
    "print(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 0) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-6cba7ee90214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# output = cosine_similarity_matrix_loop(input1, input2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 0) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "input1 = torch.randn(5, 128)\n",
    "input2 = torch.randn(5, 128)\n",
    "# output = cosine_similarity_matrix_loop(input1, input2)\n",
    "output = [[F.cosine_similarity(a, b, dim=0) for a in input1] for b in input2]\n",
    "torch.cat(output[0])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 5\n",
    "anchors = torch.randint(high=10, size=(B, 1024))\n",
    "positives = torch.randint(high=10, size=(B, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 3., 8.,  ..., 0., 8., 5.],\n",
      "        [1., 3., 6.,  ..., 4., 6., 8.],\n",
      "        [8., 2., 2.,  ..., 2., 0., 6.],\n",
      "        [3., 7., 4.,  ..., 5., 0., 8.],\n",
      "        [3., 4., 0.,  ..., 8., 8., 8.]])\n",
      "\n",
      "tensor([[1., 7., 7.,  ..., 1., 5., 8.],\n",
      "        [5., 5., 6.,  ..., 9., 2., 8.],\n",
      "        [2., 1., 7.,  ..., 7., 2., 5.],\n",
      "        [8., 8., 7.,  ..., 2., 0., 5.],\n",
      "        [4., 7., 3.,  ..., 4., 3., 1.]])\n",
      "tensor([[127.5853, 128.9341, 133.3079, 127.9922, 128.5924],\n",
      "        [129.3406, 128.7983, 129.4836, 123.9637, 128.4640],\n",
      "        [130.8587, 130.1768, 132.6311, 130.7134, 132.0984],\n",
      "        [131.9886, 131.0000, 132.9962, 126.4002, 131.0000],\n",
      "        [132.4009, 130.7746, 131.8294, 129.6302, 130.0615]])\n",
      "tensor([[127.5853, 127.9922, 128.5924, 128.9341, 133.3079],\n",
      "        [123.9637, 128.4640, 128.7983, 129.3406, 129.4836],\n",
      "        [130.1768, 130.7134, 130.8587, 132.0984, 132.6311],\n",
      "        [126.4002, 131.0000, 131.0000, 131.9886, 132.9962],\n",
      "        [129.6302, 130.0615, 130.7746, 131.8294, 132.4009]]) tensor([[0, 3, 4, 1, 2],\n",
      "        [3, 4, 1, 0, 2],\n",
      "        [1, 3, 0, 4, 2],\n",
      "        [3, 1, 4, 0, 2],\n",
      "        [3, 4, 1, 2, 0]])\n",
      "tensor([[3, 4, 1, 2],\n",
      "        [3, 4, 0, 2],\n",
      "        [1, 3, 0, 4],\n",
      "        [1, 4, 0, 2],\n",
      "        [3, 1, 2, 0]])\n",
      "tensor([3, 3, 1, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(anchors)\n",
    "print()\n",
    "print(positives)\n",
    "dists = (anchors**2).sum(dim=1).view(-1, 1) + (positives**2).sum(dim=1) - 2*anchors.matmul(positives.t())\n",
    "dists = np.sqrt(dists)\n",
    "print(dists)\n",
    "dists_sorted, dists_sorted_idx = dists.sort(dim=1)\n",
    "print(dists_sorted, dists_sorted_idx)\n",
    "tau = 0.02\n",
    "idx_threshold = round(tau * (B-1))\n",
    "# Given a distance matrix Dij, if i=j a value corresponds to a distance between \n",
    "# positive pairs -> we need to prevent them from getting to the negative samples\n",
    "# First, we need to remove i=j elements.\n",
    "mask = (dists_sorted_idx != torch.arange(B).repeat(1, B).view(B, B).t())\n",
    "dists_sorted_idx_rm = dists_sorted_idx[mask].view(B, B-1)\n",
    "print(dists_sorted_idx_rm)\n",
    "selected_idx = dists_sorted_idx_rm[:, idx_threshold]\n",
    "print(selected_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 3., 8.,  ..., 0., 8., 5.],\n",
      "        [1., 3., 6.,  ..., 4., 6., 8.],\n",
      "        [8., 2., 2.,  ..., 2., 0., 6.],\n",
      "        [3., 7., 4.,  ..., 5., 0., 8.],\n",
      "        [3., 4., 0.,  ..., 8., 8., 8.]])\n",
      "tensor([[0., 3., 8.,  ..., 0., 8., 5.],\n",
      "        [1., 3., 6.,  ..., 4., 6., 8.],\n",
      "        [8., 2., 2.,  ..., 2., 0., 6.]])\n",
      "tensor([[23., 23., 23.,  ..., 23., 23., 23.],\n",
      "        [ 1.,  3.,  6.,  ...,  4.,  6.,  8.],\n",
      "        [ 8.,  2.,  2.,  ...,  2.,  0.,  6.]])\n",
      "tensor([[23., 23., 23.,  ..., 23., 23., 23.],\n",
      "        [ 1.,  3.,  6.,  ...,  4.,  6.,  8.],\n",
      "        [ 8.,  2.,  2.,  ...,  2.,  0.,  6.],\n",
      "        [ 3.,  7.,  4.,  ...,  5.,  0.,  8.],\n",
      "        [ 3.,  4.,  0.,  ...,  8.,  8.,  8.]])\n"
     ]
    }
   ],
   "source": [
    "print(anchors)\n",
    "a = anchors[:3]\n",
    "print(a)\n",
    "a[0] = 23\n",
    "print(a)\n",
    "print(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
