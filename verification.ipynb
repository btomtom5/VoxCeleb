{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import tensorboardX\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # scipy throws future warnings on fft (known bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentificationDatasetTrain(Dataset):\n",
    "    \n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        \n",
    "        iden_split_path = os.path.join(path, 'iden_split.txt')\n",
    "        split = pd.read_table(iden_split_path, sep=' ', header=None, names=['phase', 'path'])\n",
    "        split['label'] = split['path'].apply(lambda x: int(x.split('/')[0].replace('id1', '')) - 1)\n",
    "        \n",
    "        # make train/test id split (in paths class id numbering starts with 1)\n",
    "        fullid_arr = np.arange(1251) # 1--1251\n",
    "        testid_arr = np.arange(269, 309) # 270--309\n",
    "        trainid_arr = np.setdiff1d(fullid_arr, testid_arr) # 1--1251 \\ 270--309\n",
    "        # subsetting ids for training\n",
    "        mask = split['label'].isin(trainid_arr)\n",
    "        self.dataset = split['path'][mask].reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # path\n",
    "        track_path = self.dataset[idx]\n",
    "        audio_path = os.path.join(self.path, 'audio', track_path)\n",
    "\n",
    "        # read .wav\n",
    "        rate, samples = wavfile.read(audio_path)\n",
    "        # extract label from path like id10003/L9_sh8msGV59/00001.txt\n",
    "        # subtracting 1 because PyTorch assumes that C_i in [0, 1251-1]\n",
    "        label = int(track_path.split('/')[0].replace('id1', '')) - 1\n",
    "        # PyTorch complains if label > num_classes. For ex, num_classes=1211\n",
    "        # label is 1250. train labels \\in [0, ..., 268, 309, ..., 1250]. (269 + 942 = 1211)\n",
    "        # therefore, we subtract 40 (# of test classes) from a label => label \\in [0, 1211]\n",
    "        if label >= 309:\n",
    "            label -= 40\n",
    "\n",
    "        ## parameters\n",
    "        window = 'hamming'\n",
    "        # window width and step size\n",
    "        Tw = 25 # ms\n",
    "        Ts = 10 # ms\n",
    "        # frame duration (samples)\n",
    "        Nw = int(rate * Tw * 1e-3)\n",
    "        Ns = int(rate * (Tw - Ts) * 1e-3)\n",
    "        # overlapped duration (samples)\n",
    "        # 2 ** to the next pow of 2 of (Nw - 1)\n",
    "        nfft = 2 ** (Nw - 1).bit_length()\n",
    "        pre_emphasis = 0.97\n",
    "        \n",
    "        # preemphasis filter\n",
    "        samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
    "        \n",
    "        # removes DC component of the signal and add a small dither\n",
    "        samples = signal.lfilter([1, -1], [1, -0.99], samples)\n",
    "        dither = np.random.uniform(-1, 1, samples.shape)\n",
    "        spow = np.std(samples)\n",
    "        samples = samples + 1e-6 * spow * dither\n",
    "        \n",
    "        # segment selection\n",
    "        segment_len = 3 # sec\n",
    "        upper_bound = len(samples) - segment_len * rate\n",
    "        start = np.random.randint(0, upper_bound)\n",
    "        end = start + segment_len * rate\n",
    "        samples = samples[start:end]\n",
    "        \n",
    "        # spectogram\n",
    "        _, _, spec = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
    "                                        mode='magnitude', return_onesided=False)\n",
    "        \n",
    "        # just multiplying it by 1600 makes spectrograms in the paper and here \"the same\"\n",
    "        spec *= rate / 10\n",
    "        \n",
    "        if self.transform:\n",
    "            spec = self.transform(spec)\n",
    "\n",
    "        return label, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"Normalizes voice spectrogram (mean-varience)\"\"\"\n",
    "    \n",
    "    def __call__(self, spec):\n",
    "        \n",
    "        # (Freq, Time)\n",
    "        # mean-variance normalization for every spectrogram (not batch-wise)\n",
    "        mu = spec.mean(axis=1).reshape(512, 1)\n",
    "        sigma = spec.std(axis=1).reshape(512, 1)\n",
    "        spec = (spec - mu) / sigma\n",
    "\n",
    "        return spec\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert spectogram to Tensor.\"\"\"\n",
    "    \n",
    "    def __call__(self, spec):\n",
    "        F, T = spec.shape\n",
    "        \n",
    "        # now specs are of size (Freq, Time) and 2D but has to be 3D (channel dim)\n",
    "        spec = spec.reshape(1, F, T)\n",
    "        \n",
    "        # make the ndarray to be of a proper type (was float64)\n",
    "        spec = spec.astype(np.float32)\n",
    "        \n",
    "        return torch.from_numpy(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(VoiceNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=7, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(num_features=96)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=256)\n",
    "        self.bn6 = nn.BatchNorm2d(num_features=4096)\n",
    "        self.bn7 = nn.BatchNorm1d(num_features=1024)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.mpool5 = nn.MaxPool2d(kernel_size=(5, 3), stride=(3, 2))\n",
    "        \n",
    "        # Conv2d with weights of size (H, 1) is identical to FC with H weights\n",
    "        self.fc6 = nn.Conv2d(in_channels=256, out_channels=4096, kernel_size=(9, 1))\n",
    "        self.fc7 = nn.Linear(in_features=4096, out_features=1024)\n",
    "        self.fc8 = nn.Linear(in_features=1024, out_features=num_classes)\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.mpool1(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.mpool2(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.mpool5(x)\n",
    "        x = self.relu(self.bn6(self.fc6(x)))\n",
    "        \n",
    "        _, _, _, W = x.size()\n",
    "        self.apool6 = nn.AvgPool2d(kernel_size=(1, W))\n",
    "        x = self.apool6(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.relu(self.bn7(self.fc7(x)))\n",
    "            x = self.fc8(x)\n",
    "        \n",
    "        # we use the fc7 output for Hard Negative Mining (inference)\n",
    "        else:\n",
    "            x = self.fc7(x)\n",
    "            x = F.normalize(x)\n",
    "        \n",
    "        # during training, there's no need for SoftMax because CELoss calculates it\n",
    "        return x\n",
    "    \n",
    "    # phase: [training_iden, inference_negative_mining, training_siamese, verif_test]\n",
    "    def forward(self, voice1, voice2=None, phase='train_iden'):\n",
    "        if phase in ['train_iden', 'eval_mining']:\n",
    "            return self.forward_once(voice1)\n",
    "        \n",
    "        elif phase in ['train_veri', 'eval_veri']:\n",
    "            voice1 = self.forward_once(voice1)\n",
    "            voice2 = self.forward_once(voice2)\n",
    "            return voice1, voice2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/home/nvme/data/vc1/'\n",
    "LOG_PATH = '/home/nvme/logs/VoxCeleb/verif_class'\n",
    "EPOCH_NUM = 30\n",
    "\n",
    "# in shared code B = 100 but PyTorch throws CUDA out of memory at B = 97 \n",
    "# though B=96 takes only 90.6% of the GPU Mem (bug?):\n",
    "# https://discuss.pytorch.org/t/lesser-memory-consumption-with-a-larger-batch-in-multi-gpu-setup/29087\n",
    "# B = 96\n",
    "# but when \n",
    "torch.backends.cudnn.deterministic = True\n",
    "# I can set B = 100\n",
    "B = 100\n",
    "\n",
    "WEIGHT_DECAY = 5e-4\n",
    "LR_INIT = 1e-2\n",
    "LR_LAST = 1e-4\n",
    "# lr scheduler parameter\n",
    "gamma = 10 ** (np.log10(LR_LAST / LR_INIT) / (EPOCH_NUM - 1))\n",
    "MOMENTUM = 0.9\n",
    "DEVICE = 'cuda:0'\n",
    "NUM_WORKERS = 4\n",
    "TBoard = tensorboardX.SummaryWriter(log_dir=LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VoiceNet(num_classes=1211)\n",
    "net.to(DEVICE)\n",
    "\n",
    "transforms = Compose([\n",
    "    Normalize(),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "trainset = IdentificationDatasetTrain(DATASET_PATH, transform=transforms)\n",
    "trainsetloader = torch.utils.data.DataLoader(trainset, batch_size=B, \n",
    "                                             num_workers=NUM_WORKERS, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), LR_INIT, MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1487it [07:20,  4.08it/s]\n",
      "1487it [07:25,  4.10it/s]\n",
      "1487it [07:25,  4.09it/s]\n",
      "1487it [07:25,  4.09it/s]\n",
      "1487it [07:25,  4.09it/s]\n",
      "1487it [07:25,  4.08it/s]\n",
      "1487it [07:25,  4.08it/s]\n",
      "1487it [07:25,  4.02it/s]\n",
      "1487it [07:25,  4.10it/s]\n",
      "1487it [07:25,  4.09it/s]\n",
      "1487it [07:25,  4.03it/s]\n",
      "1487it [07:25,  4.08it/s]\n",
      "1487it [07:24,  4.09it/s]\n",
      "1487it [07:25,  4.10it/s]\n",
      "18it [00:06,  3.33it/s]Process Process-59:\n",
      "Process Process-57:\n",
      "Process Process-60:\n",
      "Process Process-58:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-5-47f8540fcf47>\", line 52, in __getitem__\n",
      "    samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 104, in get\n",
      "    if timeout < 0 or not self._poll(timeout):\n",
      "  File \"<ipython-input-5-47f8540fcf47>\", line 57, in __getitem__\n",
      "    spow = np.std(samples)\n",
      "  File \"<ipython-input-5-47f8540fcf47>\", line 69, in __getitem__\n",
      "    mode='magnitude', return_onesided=False)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/scipy/signal/spectral.py\", line 707, in spectrogram\n",
      "    mode='stft')\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/numpy/core/fromnumeric.py\", line 3038, in std\n",
      "    **kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/scipy/signal/spectral.py\", line 1641, in _spectral_helper\n",
      "    result = _fft_helper(x, win, detrend_func, nperseg, noverlap, nfft, sides)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/numpy/core/_methods.py\", line 140, in _std\n",
      "    keepdims=keepdims)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/scipy/signal/spectral.py\", line 1726, in _fft_helper\n",
      "    result = func(result, n=nfft)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/numpy/core/_methods.py\", line 117, in _var\n",
      "    x = asanyarray(arr - arrmean)\n",
      "  File \"/usr/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/vladimir/venv/lib/python3.5/site-packages/scipy/fftpack/basic.py\", line 283, in fft\n",
      "    return work_function(tmp,n,1,0,overwrite_x)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b64ee9128846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# TBoard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mstep_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_num\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainsetloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0miter_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mTBoard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Metrics/train_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mTBoard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Metrics/lr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_num in range(EPOCH_NUM):\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # train\n",
    "    net.train()\n",
    "    \n",
    "    for iter_num, (labels, specs) in tqdm(enumerate(trainsetloader)):\n",
    "        optimizer.zero_grad()\n",
    "        labels, specs = labels.to(DEVICE), specs.to(DEVICE)\n",
    "        scores = net(specs)\n",
    "        loss = criterion(scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # TBoard\n",
    "        step_num = epoch_num * len(trainsetloader) + iter_num\n",
    "        TBoard.add_scalar('Metrics/train_loss', loss.item(), step_num)\n",
    "        TBoard.add_scalar('Metrics/lr', lr_scheduler.get_lr()[0], step_num)\n",
    "        \n",
    "# when the training is finished save the model\n",
    "torch.save(net.state_dict(), os.path.join(LOG_PATH, 'model_snapshot_{}.txt'.format(time.time())))\n",
    "TBoard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), os.path.join(LOG_PATH, 'model_snapshot_{}.txt'.format(time.time())))\n",
    "TBoard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dict = torch.load(os.path.join(LOG_PATH, 'model_snapshot_1542979501.519298.txt'))\n",
    "\n",
    "net = VoiceNet(num_classes=1211)\n",
    "net.to(DEVICE)\n",
    "\n",
    "model_dict = net.state_dict()\n",
    "\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict) \n",
    "# 3. load the new state dict\n",
    "net.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1211])\n",
      "torch.Size([10, 1024])\n",
      "torch.Size([10, 1024])\n",
      "torch.Size([10, 1024])\n",
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "B = 10\n",
    "\n",
    "net.train()\n",
    "v1 = torch.rand((B, 1, 512, 298)).to(DEVICE)\n",
    "print(net(v1, phase='train_iden').shape) # B, 1211\n",
    "\n",
    "net.eval()\n",
    "v1 = torch.rand((B, 1, 512, 298)).to(DEVICE)\n",
    "v2 = torch.rand((B, 1, 512, 298)).to(DEVICE)\n",
    "print(net(v1, phase='eval_mining').shape) # B, 1024\n",
    "print(net(v2, phase='eval_mining').shape) # B, 1024\n",
    "\n",
    "net.train()\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "net.fc8 = nn.Linear(net.fc8.in_features, 1024).to(DEVICE)\n",
    "v1 = torch.rand((B, 1, 512, 298)).to(DEVICE)\n",
    "v2 = torch.rand((B, 1, 512, 298)).to(DEVICE)\n",
    "print(net(v1, v2, phase='train_veri')[1].shape) # B, 1024\n",
    "\n",
    "net.eval()\n",
    "v1 = torch.rand((1, 1, 512, 2324)).to(DEVICE)\n",
    "v2 = torch.rand((1, 1, 512, 3245)).to(DEVICE)\n",
    "print(net(v1, v2, phase='eval_veri')[1].shape) # 1, 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1211\n",
      "1240\n"
     ]
    }
   ],
   "source": [
    "fullid_arr = np.arange(1251) # 1--1251\n",
    "testid_arr = np.arange(269, 309) # 270--309\n",
    "trainid_arr = np.setdiff1d(fullid_arr, testid_arr)\n",
    "print(len(trainid_arr))\n",
    "print(trainid_arr[1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardNegativeMining(Dataset):\n",
    "    \n",
    "    def __init__(self, path, model, transform=None):\n",
    "        self.path = path\n",
    "        self.model = model\n",
    "        self.transform = transform\n",
    "        \n",
    "        fullid_arr = np.arange(1251) # 1--1251\n",
    "        testid_arr = np.arange(269, 309) # 270--309\n",
    "        trainid_arr = np.setdiff1d(fullid_arr, testid_arr) # 1--1251 \\ 270--309\n",
    "        self.dataset = trainid_arr\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.dataset\n",
    "#         return (<B positives + B/2 random negatives + B/2 10% hardest> x 1 x 512 x 298)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullid_arr = np.arange(1, 1252) # 1--1251\n",
    "testid_arr = np.arange(270, 310) # 270--309\n",
    "trainid_arr = np.setdiff1d(fullid_arr, testid_arr) # 1--1251 \\ 270--309\n",
    "a = np.random.choice(trainid_arr, 1211, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  26,  568,  527,  740,  452,  785,  763, 1091, 1185,  858, 1077,\n",
       "        1221,  537,  342, 1202,  810, 1149,   40,   66,  580,  442,  691,\n",
       "         542,   15,  438,  877,  696,  500,  615,  777,  243,  635,  686,\n",
       "         140, 1208,  232, 1003,  775, 1206,  656,  311,  581,  987,   90,\n",
       "        1081,  796,  772, 1141,   36,  398,  862,  550,  385,  824,  974,\n",
       "         636,  382, 1095,  773,  392,  173,  268, 1119,  141,  692, 1164,\n",
       "         571,   68,  715,  351,  454,  754,  618,  988,  531,  989,  214,\n",
       "        1043, 1161, 1192, 1154,  887,  888,  118, 1006,  724,  943,  972,\n",
       "         595,  344, 1201,  757,  803, 1147,  933,  448,  826, 1009,   46,\n",
       "         986,  739]),\n",
       " array([ 793,  813, 1170,  870, 1108,  319, 1090,  245,  624,  714,  146,\n",
       "         162, 1059, 1159, 1085,   74, 1068,  113,  902, 1215,  980,  421,\n",
       "         657,  850,  926,  658,  579, 1033,  661,  119,  868,  126, 1246,\n",
       "          86,  770,  953,  776,  318,  328,  137, 1184,  347,  465,  735,\n",
       "         979,  600,  403,  721,  157,  525,  455,  651,  956,   71,  267,\n",
       "        1049,  504,  380,  258,  625,  732, 1205,  587,   17,  996,  517,\n",
       "         860, 1137,  889, 1165,  835, 1158,  646,  519,  269, 1052, 1005,\n",
       "         374,  942,  814, 1224, 1072,  578, 1132, 1121,  499,  128,  873,\n",
       "        1024, 1216,  844,  574,  687, 1131,  998,  805,  643, 1173,  759,\n",
       "         856,  631]),\n",
       " array([ 443,  867,  622,  538,  168,  828, 1242,  489,  707,  107, 1174,\n",
       "         761,  879,  466,  233,  855,  114,  459,  560,  475,  354,  491,\n",
       "         427,  847,  518,  240,  497,  144, 1219,  177,   29,  529,  798,\n",
       "        1063,  786, 1034,  762,  584,  616,  263,  139,  995,   80,  539,\n",
       "        1074,  698,  349,  169,  482,  109,  375,  266, 1220,  884,  161,\n",
       "           8,   95,  655,  647,  136,  341,  352,  224,   54,  644,  799,\n",
       "         890,  809, 1050,  990,  138, 1078,  666,  313,  142, 1123,  745,\n",
       "         406,  506,  336,  121,  628,   94,  133, 1032,  971,  434,  117,\n",
       "        1067,  315,  690,   41, 1011,  720,  490,  469,   59, 1188,  324,\n",
       "        1177,  217]),\n",
       " array([ 108,  472,  671,  781,  804,  508, 1230,  664,  417,  589,  833,\n",
       "         660,  819,  129,  913, 1199,   16,  925,  543,   33,  878,   53,\n",
       "         532,  612,  160,  544,  343,  339, 1133,  104, 1098,  771,  390,\n",
       "         678,  681,  368,  548,  103, 1016, 1019,  393,  203,  765,  321,\n",
       "         897,  190,  204,  209,  122,   61, 1180,   47, 1248,  437,  702,\n",
       "         553,   64,  225,  604,  171,  112, 1055, 1002,  976,  668,  940,\n",
       "         360,  208, 1175,   92,  476,  737,  405,   12, 1110,  314,  638,\n",
       "         515,  750, 1101,  898,  676,  934,  130, 1150,  893,   69,  728,\n",
       "         659,   98,  726,  811,  229,  218,  718,  450, 1014, 1152,  250,\n",
       "          49,  101]),\n",
       " array([ 957,  413,   72,   19, 1155,  514,  394,  843,  235,  738,  105,\n",
       "        1022,  609,  536,  899,   43, 1048,  501, 1010,  950, 1214, 1046,\n",
       "         384,  346,  186,  528,  312,   57,  845, 1111,  968,  132,  688,\n",
       "         365,  262, 1035,  952,  594,  670,  573,  520,  110,    5,  846,\n",
       "          77,  911,  534, 1004,   84,  749,  323,  947,  127,   82,  648,\n",
       "         640,  607,   89,  420,  900,  768,   44, 1041,  386,  184,  511,\n",
       "         802,  155,  175,  371,  836,  774,   75,    4, 1000,  764,  134,\n",
       "         178, 1007,  219, 1096,  951,  461,  653,  408, 1157,  212, 1229,\n",
       "         734,  959, 1030, 1204,  147, 1178,  875,  345,  377, 1200, 1162,\n",
       "         694,  440]),\n",
       " array([ 199,  838,  523, 1145, 1235,  510,  445,  516,  700, 1146, 1104,\n",
       "         747,  633,  760,  918,  167,    2,  742,  654,  922,  621,  945,\n",
       "         930,  936,  629,  891,  205,  821, 1025, 1107,  546,  404,  815,\n",
       "        1124,   58, 1218,  135,  675, 1017,  852,  257,  389,  152,  396,\n",
       "        1122, 1236,  717, 1169,  931, 1044,  590,  784,  249,  788,    7,\n",
       "         415,  391, 1045, 1187,  780,  350,  588,  237,  881, 1027,  464,\n",
       "         722,  983,  541, 1109,   20,  910,  200,  806,   34,  848, 1225,\n",
       "        1166,  561,   88, 1129, 1071, 1053, 1231,  795,  222, 1080, 1102,\n",
       "         503,  496,  841,  387,  915,  376,  505,   81,  713,  383,  816,\n",
       "         148,  338]),\n",
       " array([ 829,  255,   11,  547,  228, 1128, 1039, 1213,  570,  954, 1116,\n",
       "        1233,   23, 1087, 1167,  555,  123,  179,  366,  869, 1171,  468,\n",
       "         236,  684,  562,  709,  447,  977,  409,  857,  513,   78,  355,\n",
       "         746,  361,  356,  837,  960,  176,  903,  436,  189, 1018,  928,\n",
       "          21, 1143,   70,  876,  156,  592,  994,  364,  369,  234,    6,\n",
       "         483,  192,  614, 1209, 1058,  180,  451, 1001,   67,   28,  533,\n",
       "         921,  397,  769, 1064, 1023,  191,  610,  433,   73, 1040,  424,\n",
       "        1196,  530, 1026,  672, 1126, 1249,  779, 1092,  487, 1060,  997,\n",
       "         358, 1097,  458, 1251,  164,  462,  710,  198,  432,  494,   39,\n",
       "         165,  223]),\n",
       " array([ 430, 1203, 1189,  627,   51,  602,  605,  373,  522, 1038,  260,\n",
       "         613,  981,  552,  246,  901, 1086,  917, 1066,  473,  593, 1114,\n",
       "         502,  558,  964, 1194, 1105,  822,  791, 1197,  992, 1193,  577,\n",
       "         419,  935,   38,  914,  206, 1134,  449, 1076, 1140,  790,  479,\n",
       "         782,   14,   63,   87,  251, 1118,  832, 1195,  885,  667,  428,\n",
       "        1125,  958,  744,  182,  478,  399,  599,  337, 1073, 1183,  359,\n",
       "         320,  650,  456,  557,  999,  626,  673,  831,  325,  422,  498,\n",
       "        1168,  601, 1190,  316,  719, 1191,   32, 1156,  259,  723,  741,\n",
       "         669,  196,  481,  241,  820,  485,  261,  559,  444, 1120,  411,\n",
       "         712,   31]),\n",
       " array([  35,  254,  985, 1075, 1227,  149,  606,   42,  474,  166,  102,\n",
       "         767,  753, 1013,  894,   99,  965, 1037,  526,   27,  210,  778,\n",
       "         151,   85,  807,  665,  923,  871,  211,  729,  407,  326,  582,\n",
       "         608, 1054,  185,  120,  329,  730, 1130,  929, 1135,  572,  849,\n",
       "          55,  569,  706,  982, 1070,   37,  703, 1112, 1151,  253, 1056,\n",
       "         335,  912, 1065,  231,  549,   52,  801,  226,  486,  565,  674,\n",
       "         872,  683,  193,  264,   65,  920, 1226, 1127, 1094,  363, 1198,\n",
       "         100,  357,  435,  727,  949, 1083,  896, 1051,  317,  927,  961,\n",
       "         993,  446,  800,  649,  886,  453, 1179, 1182,  830,   48,  693,\n",
       "        1212,  238]),\n",
       " array([1142,  679,   13,  944,  310,  991,  916,  598,  854,  330, 1186,\n",
       "          18,  842,  467,  116, 1211, 1181,  924,  201,  470,  794, 1088,\n",
       "        1008, 1031,  908, 1245,  334, 1057,  348,  174,  353,  866,  736,\n",
       "         401,  460,  188,    9,  939,  431, 1148,  248,  758,  242, 1241,\n",
       "         865,  556,  906, 1029,  680,  213,  677, 1042,  603,  597,  507,\n",
       "         783,  163,  812,  766,  639,  955,  984,  521,  970,  154,  975,\n",
       "         731,  642,  962,   83,   25, 1028,  441,  400,  823,  973,  705,\n",
       "         586,  402,  946,  488,  701,  966,  239, 1222,  150,  905, 1228,\n",
       "         111,  652,  695,  381,  378,  817,  919,  172,  630,  410,  825,\n",
       "        1234,  859]),\n",
       " array([1069,  563, 1136,   24,  663,   30,  864,  125,  244,  197,  941,\n",
       "         265, 1238,   62,  789,  967,  457, 1062,  183,  880,  575,  247,\n",
       "         792,  412,  418, 1012,  755,  106, 1036,  551,  327,  637,  332,\n",
       "        1239, 1163,  978,  463,  194, 1250,  743,  617,  207,  509,  423,\n",
       "         512,  576,  853,   10,  591,  874,  216, 1099,  331,  752,  388,\n",
       "        1103,  689, 1020, 1223, 1139,  414,    1,  480,  124, 1117,  540,\n",
       "        1247,  370,  963,   96,  685,  839,  725,   97,  252,  619,  907,\n",
       "        1160,   50,  426, 1084, 1207,  495,  733, 1243, 1082,   60, 1115,\n",
       "         493,  748,  143,  524,  181,  566,  895,  682,  620,    3,  379,\n",
       "         851,  938]),\n",
       " array([ 227,  892,  372, 1106, 1093,  340,  704, 1021,  170,  395,  416,\n",
       "         564,  567,  187,  697,  834,  909,  937,  932, 1244,  195, 1047,\n",
       "        1153, 1237,  708,  904,  818, 1210, 1232, 1240, 1172,  699,  367,\n",
       "        1176,   56,  634,  641,   76, 1113,  492,  611,  797,  429,  583,\n",
       "          22,  883,  711,   79,  632,  215,  256, 1061, 1138,  230, 1079,\n",
       "         535,  131,  220,  221,  827,  333,  484,  787,  596,  756,  202,\n",
       "         861, 1100,  425,  840,  716, 1015,   45,  471,  322,  645,  362,\n",
       "         439,  751,   91,  158,  863,  948,  554,  969,   93,  662,  882,\n",
       "         545,  623,  145, 1089,  477,  153, 1217,  585,  115,  159, 1144,\n",
       "         808])]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_split(a, 1211 // 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationDatasetTrain(Dataset):\n",
    "    \n",
    "    def __init__(self, path, model, batch_size, transform=None):\n",
    "        self.path = path\n",
    "        self.model = model\n",
    "        self.transform = transform\n",
    "        \n",
    "        fullid_arr = np.arange(1, 1252) # 1--1251\n",
    "        testid_arr = np.arange(270, 310) # 270--309\n",
    "        trainid_arr = np.setdiff1d(fullid_arr, testid_arr) # 1--1251 \\ 270--309\n",
    "        \n",
    "        # split the set of ids into `len(trainid_arr) // batch_size` subsets\n",
    "        splits = np.array_split(trainid_arr, len(trainid_arr) // batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
